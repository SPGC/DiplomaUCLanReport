\begin{thebibliography}{}

\bibitem[Avidan and Shamir, 2007]{seamcarving}
Avidan, S. and Shamir, A. (2007).
\newblock Seam carving for content-aware image resizing.
\newblock {\em SIGGRAPH}, 26.

\bibitem[Chen et~al., 2017]{deeplabv3}
Chen, L.-C., Papandreou, G., Schroff, F., and Adam, H. (2017).
\newblock Rethinking atrous convolution for semantic image segmentation.

\bibitem[Dellaert, 2003]{em_algo_2}
Dellaert, F. (2003).
\newblock The expectation maximization algorithm.

\bibitem[Dosovitskiy et~al., 2021]{transformers}
Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X.,
  Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S.,
  Uszkoreit, J., and Houlsby, N. (2021).
\newblock An image is worth 16x16 words: Transformers for image recognition at
  scale.

\bibitem[Guo et~al., 2003]{knn}
Guo, G., Wang, H., Bell, D., Bi, Y., and Greer, K. (2003).
\newblock Knn model-based approach in classification.
\newblock volume 2888, pages 986--996.

\bibitem[Howard et~al., 2019]{mobilenetv3}
Howard, A., Sandler, M., Chu, G., Chen, L.-C., Chen, B., Tan, M., Wang, W.,
  Zhu, Y., Pang, R., Vasudevan, V., Le, Q.~V., and Adam, H. (2019).
\newblock Searching for mobilenetv3.

\bibitem[Howard et~al., 2017]{mobileNet}
Howard, A.~G., Zhu, M., Chen, B., Kalenichenko, D., Wang, W., Weyand, T.,
  Andreetto, M., and Adam, H. (2017).
\newblock Mobilenets: Efficient convolutional neural networks for mobile vision
  applications.

\bibitem[Hu et~al., 2019]{seNet}
Hu, J., Shen, L., Albanie, S., Sun, G., and Wu, E. (2019).
\newblock Squeeze-and-excitation networks.

\bibitem[Ioffe and Szegedy, 2015]{batch_norm}
Ioffe, S. and Szegedy, C. (2015).
\newblock Batch normalization: Accelerating deep network training by reducing
  internal covariate shift.

\bibitem[Kaur, 2024]{adam_vs_sgd}
Kaur, H. (2024).
\newblock Why adam optimizer should not be the default learning algorithm.
\newblock
  https://pub.towardsai.net/why-adam-optimizer-should-not-be-the-default-learning-algorithm-a2b8d019eaa0.
\newblock Accessed: February 21, 2024.

\bibitem[Kennedy, 2018]{go_shedulers}
Kennedy, W. (2018).
\newblock Scheduling in go : Part ii - go scheduler.
\newblock https://www.ardanlabs.com/blog/2018/08/scheduling-in-go-part2.html.
\newblock Accessed: February 21, 2024.

\bibitem[Kingma and Ba, 2017]{adam}
Kingma, D.~P. and Ba, J. (2017).
\newblock Adam: A method for stochastic optimization.

\bibitem[Krizhevsky et~al., 2012]{alexnet}
Krizhevsky, A., Sutskever, I., and Hinton, G.~E. (2012).
\newblock Imagenet classification with deep convolutional neural networks.
\newblock In {\em Proceedings of the 25th International Conference on Neural
  Information Processing Systems - Volume 1}, NIPS'12, page 1097–1105, Red
  Hook, NY, USA. Curran Associates Inc.

\bibitem[Long et~al., 2015]{FCN}
Long, J., Shelhamer, E., and Darrell, T. (2015).
\newblock Fully convolutional networks for semantic segmentation.

\bibitem[Ng et~al., 2004]{em1}
Ng, S., Krishnan, T., and Mclachlan, G. (2004).
\newblock The em algorithm.
\newblock {\em Handbook of Computational Statistics: Concepts and Methods}.

\bibitem[Nikolenko, 2019]{synthetic_data}
Nikolenko, S.~I. (2019).
\newblock Synthetic data for deep learning.

\bibitem[O'Shea and Nash, 2015]{cnn}
O'Shea, K. and Nash, R. (2015).
\newblock An introduction to convolutional neural networks.

\bibitem[Peng et~al., 2002]{log_regression}
Peng, J., Lee, K., and Ingersoll, G. (2002).
\newblock An introduction to logistic regression analysis and reporting.
\newblock {\em Journal of Educational Research - J EDUC RES}, 96:3--14.

\bibitem[Perez and Wang, 2017]{augmentations}
Perez, L. and Wang, J. (2017).
\newblock The effectiveness of data augmentation in image classification using
  deep learning.

\bibitem[Podpora et~al., 2014]{YCbCr}
Podpora, M., Korba´s, G., and Kawala-Janik, A. (2014).
\newblock Yuv vs rgb – choosing a color space for human-machine interaction.
\newblock {\em Annals of Computer Science and Information Systems}, Vol. 3.

\bibitem[Prove, 2017]{conv2dParamsArticle}
Prove, P.-L. (2017).
\newblock An introduction to different types of convolutions in deep learning.
\newblock
  https://towardsdatascience.com/types-of-convolutions-in-deep-learning-717013397f4d.
\newblock Accessed: February 19, 2024.

\bibitem[PyTorchContributors, 2023]{conv2dTorch}
PyTorchContributors (2023).
\newblock Pytorch docs.
\newblock https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html.
\newblock Accessed: February 19, 2024.

\bibitem[Raschka et~al., 2020]{pythonForAI}
Raschka, S., Patterson, J., and Nolet, C. (2020).
\newblock Machine learning in python: Main developments and technology trends
  in data science, machine learning, and artificial intelligence.
\newblock {\em Information}, 11(4).

\bibitem[Ruder, 2017]{sgd}
Ruder, S. (2017).
\newblock An overview of gradient descent optimization algorithms.

\bibitem[Sandler et~al., 2019]{mobile_netv2}
Sandler, M., Howard, A., Zhu, M., Zhmoginov, A., and Chen, L.-C. (2019).
\newblock Mobilenetv2: Inverted residuals and linear bottlenecks.

\bibitem[Scabini and Bruno, 2021]{fully_connected}
Scabini, L. F.~S. and Bruno, O.~M. (2021).
\newblock Structure and performance of fully connected neural networks:
  Emerging complex network properties.

\bibitem[Sniffin, 2022]{goroutines}
Sniffin, A. (2022).
\newblock Applications of goroutines \& channels in go.
\newblock
  https://alexsniffin.medium.com/applications-of-goroutines-channels-in-go-2e24c478d71d.
\newblock Accessed: February 21, 2024.

\bibitem[Snyk, 2024]{mit}
Snyk (2024).
\newblock What is the mit license? top 10 questions answered.
\newblock https://snyk.io/blog/mit-license-explained/.
\newblock Accessed: February 21, 2024.

\bibitem[Srivastava et~al., 2014]{dropout}
Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., and Salakhutdinov,
  R. (2014).
\newblock Dropout: A simple way to prevent neural networks from overfitting.
\newblock {\em Journal of Machine Learning Research}, 15(56):1929--1958.

\bibitem[Sun et~al., 2017]{big_data_less_noice}
Sun, C., Shrivastava, A., Singh, S., and Gupta, A. (2017).
\newblock Revisiting unreasonable effectiveness of data in deep learning era.

\bibitem[Tan and Le, 2020]{efficientnet}
Tan, M. and Le, Q.~V. (2020).
\newblock Efficientnet: Rethinking model scaling for convolutional neural
  networks.

\bibitem[Tomar and Laxkar, 2022]{relu}
Tomar, A. and Laxkar, P. (2022).
\newblock Differences of tanh, sigmoid and relu activation function in neural
  network.
\newblock {\em International Journal of Scientific Progress and Research},
  80(6).

\bibitem[Zhang et~al., 2017]{shufflenet}
Zhang, X., Zhou, X., Lin, M., and Sun, J. (2017).
\newblock Shufflenet: An extremely efficient convolutional neural network for
  mobile devices.

\end{thebibliography}
