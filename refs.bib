@inproceedings{alexnet,
author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
title = {ImageNet Classification with Deep Convolutional Neural Networks},
year = {2012},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5\% and 17.0\% which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully-connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overriding in the fully-connected layers we employed a recently-developed regularization method called "dropout" that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3\%, compared to 26.2\% achieved by the second-best entry.},
booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 1},
pages = {1097–1105},
numpages = {9},
location = {Lake Tahoe, Nevada},
series = {NIPS'12}
}

@misc{big_data_less_noice,
      title={Revisiting Unreasonable Effectiveness of Data in Deep Learning Era}, 
      author={Chen Sun and Abhinav Shrivastava and Saurabh Singh and Abhinav Gupta},
      year={2017},
      eprint={1707.02968},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{augmentations,
      title={The Effectiveness of Data Augmentation in Image Classification using Deep Learning}, 
      author={Luis Perez and Jason Wang},
      year={2017},
      eprint={1712.04621},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{efficientnet,
      title={EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks}, 
      author={Mingxing Tan and Quoc V. Le},
      year={2020},
      eprint={1905.11946},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{em_algo_2,
author = {Dellaert, Frank},
year = {2003},
month = {07},
pages = {},
title = {The Expectation Maximization Algorithm}
}

@inproceedings{knn,
author = {Guo, Gongde and Wang, Hui and Bell, David and Bi, Yaxin and Greer, Kieran},
year = {2003},
month = {01},
pages = {986-996},
title = {KNN Model-Based Approach in Classification},
volume = {2888},
isbn = {978-3-540-20498-5},
journal = {Lect Notes Comput Sci},
doi = {10.1007/978-3-540-39964-3_62}
}


@misc{mobileNet,
      title={MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications}, 
      author={Andrew G. Howard and Menglong Zhu and Bo Chen and Dmitry Kalenichenko and Weijun Wang and Tobias Weyand and Marco Andreetto and Hartwig Adam},
      year={2017},
      eprint={1704.04861},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@article{seamcarving,
author = {Avidan, Shai and Shamir, Ariel},
year = {2007},
month = {07},
pages = {},
title = {Seam Carving for Content-Aware Image Resizing},
volume = {26},
journal = {SIGGRAPH},
doi = {10.1145/1276377.1276390}
}

@misc{shufflenet,
      title={ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices}, 
      author={Xiangyu Zhang and Xinyu Zhou and Mengxiao Lin and Jian Sun},
      year={2017},
      eprint={1707.01083},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{synthetic_data,
      title={Synthetic Data for Deep Learning}, 
      author={Sergey I. Nikolenko},
      year={2019},
      eprint={1909.11512},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{seNet,
      title={Squeeze-and-Excitation Networks}, 
      author={Jie Hu and Li Shen and Samuel Albanie and Gang Sun and Enhua Wu},
      year={2019},
      eprint={1709.01507},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@article{em1,
author = {Ng, See and Krishnan, Thriyambakam and Mclachlan, G.},
year = {2004},
month = {01},
pages = {},
title = {The EM algorithm},
journal = {Handbook of Computational Statistics: Concepts and Methods},
doi = {10.1007/978-3-642-21551-3_6}
}

@misc{transformers,
      title={An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale}, 
      author={Alexey Dosovitskiy and Lucas Beyer and Alexander Kolesnikov and Dirk Weissenborn and Xiaohua Zhai and Thomas Unterthiner and Mostafa Dehghani and Matthias Minderer and Georg Heigold and Sylvain Gelly and Jakob Uszkoreit and Neil Houlsby},
      year={2021},
      eprint={2010.11929},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@article{YCbCr,
author = {Podpora, Michal and Korba´s, Grzegorz and Kawala-Janik, Aleksandra},
year = {2014},
month = {09},
pages = {},
title = {YUV vs RGB – Choosing a Color Space for Human-Machine Interaction},
volume = {Vol. 3},
journal = {Annals of Computer Science and Information Systems},
doi = {10.15439/2014F206}
}

@misc{mobile_netv2,
      title={MobileNetV2: Inverted Residuals and Linear Bottlenecks}, 
      author={Mark Sandler and Andrew Howard and Menglong Zhu and Andrey Zhmoginov and Liang-Chieh Chen},
      year={2019},
      eprint={1801.04381},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{fully_connected,
      title={Structure and Performance of Fully Connected Neural Networks: Emerging Complex Network Properties}, 
      author={Leonardo F. S. Scabini and Odemir M. Bruno},
      year={2021},
      eprint={2107.14062},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{cnn,
      title={An Introduction to Convolutional Neural Networks}, 
      author={Keiron O'Shea and Ryan Nash},
      year={2015},
      eprint={1511.08458},
      archivePrefix={arXiv},
      primaryClass={cs.NE}
}

@article{log_regression,
author = {Peng, Joanne and Lee, Kuk and Ingersoll, Gary},
year = {2002},
month = {09},
pages = {3-14},
title = {An Introduction to Logistic Regression Analysis and Reporting},
volume = {96},
journal = {Journal of Educational Research - J EDUC RES},
doi = {10.1080/00220670209598786}
}

@misc{batch_norm,
      title={Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift}, 
      author={Sergey Ioffe and Christian Szegedy},
      year={2015},
      eprint={1502.03167},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{pythonForAI,
author = {Raschka, Sebastian and Patterson, Joshua and Nolet, Corey},
title = {Machine Learning in Python: Main Developments and Technology Trends in Data Science, Machine Learning, and Artificial Intelligence},
journal = {Information},
volume = {11},
year = {2020},
number = {4},
article-number = {193},
url = {https://www.mdpi.com/2078-2489/11/4/193},
issn = {2078-2489},
abstract = {Smarter applications are making better use of the insights gleaned from data, having an impact on every industry and research discipline. At the core of this revolution lies the tools and the methods that are driving it, from processing the massive piles of data generated each day to learning from and taking useful action. Deep neural networks, along with advancements in classical machine learning and scalable general-purpose graphics processing unit (GPU) computing, have become critical components of artificial intelligence, enabling many of these astounding breakthroughs and lowering the barrier to adoption. Python continues to be the most preferred language for scientific computing, data science, and machine learning, boosting both performance and productivity by enabling the use of low-level libraries and clean high-level APIs. This survey offers insight into the field of machine learning with Python, taking a tour through important topics to identify some of the core hardware and software paradigms that have enabled it. We cover widely-used libraries and concepts, collected together for holistic comparison, with the goal of educating the reader and driving the field of Python machine learning forward.},
doi = {10.3390/info11040193}
}

@misc{mobilenetv3,
      title={Searching for MobileNetV3}, 
      author={Andrew Howard and Mark Sandler and Grace Chu and Liang-Chieh Chen and Bo Chen and Mingxing Tan and Weijun Wang and Yukun Zhu and Ruoming Pang and Vijay Vasudevan and Quoc V. Le and Hartwig Adam},
      year={2019},
      eprint={1905.02244},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{FCN,
      title={Fully Convolutional Networks for Semantic Segmentation}, 
      author={Jonathan Long and Evan Shelhamer and Trevor Darrell},
      year={2015},
      eprint={1411.4038},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{deeplabv3,
      title={Rethinking Atrous Convolution for Semantic Image Segmentation}, 
      author={Liang-Chieh Chen and George Papandreou and Florian Schroff and Hartwig Adam},
      year={2017},
      eprint={1706.05587},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{conv2dTorch,
    title = {PyTorch docs},
    author = {PyTorchContributors},
    year = {2023},      
    url = {https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html},
    howpublished = {https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html},
    note = {Accessed: February 19, 2024}
}

@misc{conv2dParamsArticle,
    title = {An Introduction to different Types of Convolutions in Deep Learning},
    author = {Paul-Louis Prove},
    year = {2017},
    url = {https://towardsdatascience.com/types-of-convolutions-in-deep-learning-717013397f4d},
    howpublished = {https://towardsdatascience.com/types-of-convolutions-in-deep-learning-717013397f4d},
    note = {Accessed: February 19, 2024}
}

@article{relu,
  author          = {Archana Tomar and Pradeep Laxkar},
  journal         = {International Journal of Scientific Progress and Research},
  number          = {6},
  title           = {Differences of Tanh, sigmoid and ReLu Activation Function in Neural network},
  volume          = {80},
  year            = {2022}
}

@article{dropout,
  author  = {Nitish Srivastava and Geoffrey Hinton and Alex Krizhevsky and Ilya Sutskever and Ruslan Salakhutdinov},
  title   = {Dropout: A Simple Way to Prevent Neural Networks from Overfitting},
  journal = {Journal of Machine Learning Research},
  year    = {2014},
  volume  = {15},
  number  = {56},
  pages   = {1929--1958},
  url     = {http://jmlr.org/papers/v15/srivastava14a.html}
}

@online{mit,
  title = {What is the MIT License? Top 10 questions answered},
  author = {Snyk},
  year = {2024},
  url = {https://snyk.io/blog/mit-license-explained/},
  howpublished = {https://snyk.io/blog/mit-license-explained/},
  note = {Accessed: February 21, 2024}
}

@misc{adam,
      title={Adam: A Method for Stochastic Optimization}, 
      author={Diederik P. Kingma and Jimmy Ba},
      year={2017},
      eprint={1412.6980},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{sgd,
      title={An overview of gradient descent optimization algorithms}, 
      author={Sebastian Ruder},
      year={2017},
      eprint={1609.04747},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{adam_vs_sgd,
    author = "Harjot Kaur",
    title = "Why Adam Optimizer Should Not Be the Default Learning Algorithm",
    year = "2024",
    howpublished = "https://pub.towardsai.net/why-adam-optimizer-should-not-be-the-default-learning-algorithm-a2b8d019eaa0",
    publisher = "Towards AI",
    note = "Accessed: February 21, 2024"
}

@misc{go_shedulers,
  author = {William Kennedy},
  title = {Scheduling In Go : Part II - Go Scheduler},
  year = {2018},
  howpublished = "https://www.ardanlabs.com/blog/2018/08/scheduling-in-go-part2.html",
  publisher = "Ardanlabs",
  note = "Accessed: February 21, 2024"
}

@misc{goroutines,
  author = {Alex Sniffin},
  title = {Applications of Goroutines \& Channels in Go},
  year = {2022},
  howpublished = "https://alexsniffin.medium.com/applications-of-goroutines-channels-in-go-2e24c478d71d",
  note = "Accessed: February 21, 2024",
  publisher = "Medium"
}